# 2.2.2. 重線形回帰と正則化

## 導入

前回は$$y=ax$$という非常に簡単なモデルを扱ったが、では切片を追加して

$$
y = ax + b \tag{2.2.2.1}
$$

としたらどうなるだろうか。また、もっと入力を増やして

$$
y = w _ 0 + w _ 1 x _ 1 + w _ 2 x _ 2 + \cdots + w _ n x _ n \tag{2.2.2.2}
$$

としたらどうだろう。つまり出力$$y$$を決めうるファクターとしての入力が$$x = (x _ 1, x _ 2, \ldots, x _ n)$$のように複数ある状況を想定している。

今回は線形回帰モデルを拡張した重線形回帰モデルを扱うことにしよう。

## モデリング

実は入力を増やすだけであればやることはほとんど変わらない。$$(2.2.2.2)$$式は$$(2.2.2.1)$$式の拡張になっているから、$$(2.2.2.2)$$式についてだけ議論すればよく、前回と同様にガウス分布に従う誤差$$\varepsilon$$を導入して

$$
y = w _ 0 + w _ 1 x _ 1 + w _ 2 x _ 2 + \cdots + w _ n x _ n + \varepsilon \tag{2.2.2.3}
$$

とすればよい。やることがなさすぎるので、$$(2.2.2.3)$$式の見栄えを少しだけよくしていこう。

まず$$(2.2.2.3)$$式で切片$$w _ 0$$の項には$$x$$が現れないのでアンバランスである。そこで

$$
\phi _ i (x) = \begin{cases}
1 & {\rm if} \,\, i = 0 \\
x _ i & {\rm otherwise}
\end{cases} \tag{2.2.2.4}
$$

とおけば

$$
\begin{aligned}
y &= w _ 0 \phi _ 0 (x) + w _ 1 \phi _ 1 (x) + w _ 2 \phi _ 2 (x) + \cdots + w _ n \phi _ n (x) + \varepsilon \\
&= \sum _ {i=0} ^ n w _ i \phi _ i (x) + \varepsilon \\
&= w ^ \mathrm{T} \phi(x) + \varepsilon
\end{aligned} \tag{2.2.2.5}
$$

と書ける。上式の最後は**行列**（matrix）による表記である。

前回の内容と並べて

$$
\begin{aligned}
y &= ax + \varepsilon \\
y &= w ^ \mathrm{T} \phi(x) + \varepsilon
\end{aligned}
$$

と書けば、とても構造が似通った問題であることがわかる。

$$
p(y | x, w) = \mathcal{N}(y | w ^ \mathrm{T} \phi(x), \sigma ^ 2)
$$

$$
p(y, x, w) = p(y|x,w)p(x)p(w)
$$

$$
p(y, w|x)p(x) = p(y|x,w)p(x)p(w) \\
p(y,w|x)=p(y|x,w)p(w)
$$

$$
\underset{a \in \mathbb{R}}{\operatorname{arg} \operatorname{max}} \,\,p (\mathcal{D})
$$

## 正則化

$$
\underset{a \in \mathbb{R}}{\operatorname{arg} \operatorname{max}} \,\, p(\mathcal{D} _ y ,w | \mathcal{D} _ x)=\underset{a \in \mathbb{R}}{\operatorname{arg} \operatorname{max}} \,\, p(\mathcal{D} _ y | \mathcal{D} _ x, w)p (w)
$$

