# 2.2.2. 線形回帰モデルと正則化

## 導入

前回は$$y=ax$$という非常に簡単なモデルを扱ったが、では切片を追加して

$$
y = ax + b \tag{2.2.2.1}
$$

としたらどうなるだろうか。また、もっと入力を増やして

$$
y = w _ 0 + w _ 1 x _ 1 + w _ 2 x _ 2 + \cdots + w _ n x _ n \tag{2.2.2.2}
$$

としたらどうだろう。つまり出力$$y$$を決めうるファクターとしての入力が$$x = (x _ 1, x _ 2, \ldots, x _ n)$$のように複数ある状況を想定している。

今回は線形回帰モデルを扱うことにしよう。

## モデリング

実は入力を増やすだけであればやることはほとんど変わらない。$$(2.2.2.2)$$式は$$(2.2.2.1)$$式の拡張になっているから、$$(2.2.2.2)$$式についてだけ議論すればよく、前回と同様にガウス分布に従う誤差$$\varepsilon$$を導入して

$$
y = w _ 0 + w _ 1 x _ 1 + w _ 2 x _ 2 + \cdots + w _ n x _ n + \varepsilon \tag{2.2.2.3}
$$

とすればよい。出力$$y$$を$$(2.2.2.3)$$式で表現するモデルを**線形回帰モデル**（linear regression model）という。やることは前回『2.2.1. 線形回帰の基礎』とほとんど同じでやることがなさすぎるので、ついでに$$(2.2.2.3)$$式を今後使いやすい形に整えておこう。

まず$$(2.2.2.3)$$式で切片$$w _ 0$$の項には$$x$$が現れないのでアンバランスである。そこで

$$
\phi _ i (x) = \begin{cases}
1 & {\rm if} \,\, i = 0 \\
x _ i & {\rm otherwise}
\end{cases} \tag{2.2.2.4}
$$

とおく。今後は煩雑化を防ぐために$$\phi _ i ^ {(j)} = \phi _ i (x ^ {(j)} )$$という書き換えを断りなく用いることがある。このとき$$(2.2.2.3)$$式は

$$
\begin{aligned}
y &= w _ 0 \phi _ 0 (x) + w _ 1 \phi _ 1 (x) + w _ 2 \phi _ 2 (x) + \cdots + w _ n \phi _ n (x) + \varepsilon \\
&= \sum _ {i=0} ^ n w _ i \phi _ i (x) + \varepsilon \\
&= w ^ \mathrm{T} \phi(x)  + \varepsilon
\end{aligned} \tag{2.2.2.5}
$$

と書ける。上式の$$w ^ \mathrm{T} \phi(x)$$は**行列**（matrix）による表記である。つまり

$$
y = (
w _ 0 \,\,\, w _ 1 \,\,\, w_ 2 \, \cdots \, w _ n ) \left(
\begin{matrix}
\phi _ 0 (x) \\
\phi _ 1 (x) \\
\phi _ 2 (x) \\
\vdots \\
\phi _ n (x)
\end{matrix}
\right) + \varepsilon \tag{2.2.2.6}
$$

である。前回の内容と並べて

$$
\begin{aligned}
y &= ax + \varepsilon \\
y &= w ^ \mathrm{T} \phi(x) + \varepsilon
\end{aligned} \tag{2.2.2.7}
$$

と書けば、とても構造が似通った問題だとわかる。前回と同様にノイズを

$$
\varepsilon \sim \mathcal{N} (\varepsilon | 0, \sigma ^ 2) \tag{2.2.2.8}
$$

とおけば、

$$
p(y | x, w) = \mathcal{N}(y | w ^ \mathrm{T} \phi(x), \sigma ^ 2) \tag{2.2.2.9}
$$

となるので、尤度関数は

$$
\mathcal{L}(w|\mathcal{D}) = \prod _ {i=1} ^ N \frac{1}{\sqrt{2 \pi \sigma ^ 2}} \exp \left(- \frac{(y ^ {(i)} - w ^ \mathrm{T} \phi (x ^ {(i)})) ^ 2}{2 \sigma ^ 2} \right) \tag{2.2.2.10}
$$

である。最尤推定の文脈で解くべき最適化問題は

$$
\underset{w \in \mathbb{R} ^ {n+1}}{\operatorname{arg} \operatorname{max}} \,\, \mathcal{L} (w | \mathcal{D})
$$

となる。

### 最適化問題の解法

今回の尤度関数は多変数関数である。一応、丁寧に書いておけば

$$
\mathcal{L}(w | \mathcal{D}) = \mathcal{L}(w _ 0, w _ 1, w _ 2 , \ldots, w _ n | \mathcal{D}) \tag{2.2.2.11}
$$

である。求めるべき変数は増えたがせいぜい2次関数を多変数に拡張したものにすぎないので、方針は前回と変わらず微分して 0 になる点を探すことになる。まずはさくさくと対数尤度関数を用いた最適化問題に変形してしまおう。途中計算は前回とまったく同様なのでごっそり省くが、

$$
\begin{aligned}
\underset{w \in \mathbb{R} ^ {n+1}}{\operatorname{arg} \operatorname{max}} \,\, \ln \mathcal{L}(w | \mathcal {D}) &= \underset{w \in \mathbb{R} ^ {n+1}}{\operatorname{arg} \operatorname{max}} \,\, \left\{ -\sum _ {i=1} ^ N (y ^ {(i)}- w ^ \mathrm{T} \phi(x^{(i)})) ^ 2 \right\} \\
&=\underset{w \in \mathbb{R} ^ {n+1}}{\operatorname{arg} \operatorname{min}} \,\, \left\{ \sum _ {i=1} ^ N (y ^ {(i)} - w ^ \mathrm{T} \phi(x^{(i)})) ^ 2 \right\}
\end{aligned} \tag{2.2.2.12}
$$

である。上式は行列を用いてもっと綺麗に書き直せる。つまり$$N$$次元ベクトル$$a = (a _ 1, a _ 2, \ldots, a_N) ^ \mathrm{T}$$について一般に

$$
\| a \| ^ 2 =  a ^ \mathrm{T} a = \sum _ {i = 1}  ^ N a _ i ^ 2
$$

が成り立つことを用いればよい。いま

$$
y ^ {(i)} - w ^ \mathrm{T} \phi(x ^ {(i)})
$$

の部分は、

$$
y ^ {(i)} - \phi(x ^ {(i)}) ^ \mathrm{T}  w
$$

と書いても同じことであるから、各データ点について式を列挙すれば

$$
\left(
\begin{matrix}
y ^ {(1)} - \phi (x ^ {(1)}) ^ \mathrm{T}  w \\
y ^ {(2)} - \phi (x ^ {(2)}) ^ \mathrm{T}  w \\
\vdots \\
y ^ {(N)} - \phi (x ^ {(N)}) ^ \mathrm{T}  w
\end{matrix}
\right)
$$

であり、$$\phi _ i ^ {(j)} = \phi _ i (x ^ {(j)} )$$の略記と

$$
y = \left(
\begin{matrix}
y ^ {(1)} \\
y ^ {(2)} \\
\vdots \\
y ^ {(N)}
\end{matrix}
\right), \quad \Phi ^ \mathrm{T} = \left(
\begin{matrix}
\phi _ 0 ^ {(1)} & \phi _ 1 ^ {(1)} & \cdots & \phi _ n ^ {(1)} \\
\phi _ 0 ^ {(2)} & \phi _ 1 ^ {(2)} & \cdots & \phi _ n ^ {(2)} \\
\vdots \\
\phi _ 0 ^ {(N)} & \phi _ 1 ^ {(N)} & \cdots & \phi _ n ^ {(N)} \\
\end{matrix}
\right), \quad w = \left(
\begin{matrix}
w ^ {(0)} \\
w ^ {(1)} \\
\vdots \\
w ^ {(n)}
\end{matrix}
\right)
$$

を用いれば、

$$
\left(
\begin{matrix}
y ^ {(1)} - \phi (x ^ {(1)}) ^ \mathrm{T}  w \\
y ^ {(2)} - \phi (x ^ {(2)}) ^ \mathrm{T}  w \\
\vdots \\
y ^ {(N)} - \phi (x ^ {(N)}) ^ \mathrm{T}  w
\end{matrix}
\right) = y - \Phi ^ \mathrm{T} w
$$

と書ける。したがって$$(2.2.2.12)$$式は

$$
\underset{w \in \mathbb{R} ^ {n+1}}{\operatorname{arg} \operatorname{max}} \,\, \ln \mathcal{L}(w | \mathcal {D}) =\underset{w \in \mathbb{R} ^ {n+1}}{\operatorname{arg} \operatorname{min}} \,\, \| y - \Phi ^ \mathrm{T} w\| ^ 2
$$

と非常にシンプルな式で記述できる。

ではこの問題を解いていこう。あらためて目的関数を

$$
f(w) = \| y - \Phi ^ \mathrm{T} w\| ^ 2
$$

とおく。多変数関数なので偏微分（勾配ベクトル）が 0 ベクトルになる場所が最小値である。勾配ベクトル$$\nabla f(w)$$は

$$
\nabla f(w) = - 2 \Phi(y - \Phi ^ \mathrm{T} w)
$$

であるから、これが 0 ベクトルに等しいとき、

$$
\Phi \Phi ^ \mathrm{T} w = \Phi y
$$

となる。上式を線形回帰の**正規方程式**（normal equation）という。正規方程式は$$\Phi \Phi ^ \mathrm{T} $$が正則なとき、ただそのときに限り唯一の解

$$
w = (\Phi \Phi ^ \mathrm{T}) ^ {-1} \Phi y
$$

を持つ。これで$$\Phi \Phi ^ \mathrm{T} $$が正則なときは重みベクトルが求まった。

## 正則化

さて、次は当然の疑問として「$$\Phi \Phi ^ \mathrm{T} $$が正則でないとき重みベクトルはどう求めればよいのか」と考えるだろう。結論から言えば、正則でないならば無理やり正則にしてしまえばよい。目的関数$$g(w)$$に狭義単調増加関数$$\Psi$$を付け加えた関数

$$
f(w) = g(w) + \Psi( \|w \|)
$$

を新たな目的関数とする操作を**正則化**（regularization）という。いきなりややこしくなったので少しずつよく使われる形にしていこう。

まず知っておいてほしいのが、**ノルム**（norm）の概念だ。聞き慣れない言葉かもしれないが、ラテン語のノルマ（norma：定規のこと）が英語になったもので「長さを測るもの」をイメージしてもらえればよく、ノルムはその名の通りベクトルの長さを測る数学の道具である。

$$
p(y, x, w) = p(y|x,w)p(x)p(w)
$$

$$
p(y, w|x)p(x) = p(y|x,w)p(x)p(w) \\
p(y,w|x)=p(y|x,w)p(w)
$$

$$
\underset{a \in \mathbb{R}}{\operatorname{arg} \operatorname{max}} \,\,p (\mathcal{D})
$$

$$
\underset{a \in \mathbb{R}}{\operatorname{arg} \operatorname{max}} \,\, p(\mathcal{D} _ y ,w | \mathcal{D} _ x)=\underset{a \in \mathbb{R}}{\operatorname{arg} \operatorname{max}} \,\, p(\mathcal{D} _ y | \mathcal{D} _ x, w)p (w)
$$

