# 2.2.3. 線形基底関数モデル

今まで扱ってきた線形回帰や重線形回帰は直線や平面に合わせることしかできず、現実世界の非線形なモデルを扱うには表現力が乏しすぎる。

ニューラルネットについて知りたい人は今回を読んだあとで

{% page-ref page="neural-network.md" %}

まで飛ばしてよい。

## 線形基底関数モデル

実は前回の線形回帰モデルから変更点はひとつしかない。前回は「切片の項がアンバランスだから」という理由で$$\phi(x)$$という関数を定義したが、実は線形基底関数モデルを見越してのことである。今回はこの$$\phi(x)$$を任意の写像とするだけであり、解法や正則化の議論はまったく変わらない。

説明おしまい。

と締めくくってはあまりにも不親切なので、今回はカーネル回帰とガウス過程回帰への接続を行っていく。

線形回帰モデルで扱っていた関数$$\phi \colon \mathbb{R} ^ n \to \mathbb{R} ^ {n+1}$$は

$$
\begin{aligned}
\phi (x) &= (\phi _ 0 (x), \phi_1(x), \cdots , \phi _n(x)) ^ \mathrm{T} \\
&\phi _ i (x) = \begin{cases}
1 & {\rm if} \,\, i = 0 \\
x _ i & {\rm otherwise}
\end{cases}
\end{aligned} \tag{2.2.2.4}
$$

であった。線形基底関数モデルでは任意の写像$$\phi \colon \mathcal{X} \to \mathbb{R} ^ n$$を用いてよい。すなわち個々の写像は$$\phi _ i \colon \mathcal{X} \to \mathbb{R}$$で

$$
\phi (x) = (\phi _ 1 (x), \phi_2(x), \cdots , \phi _n(x)) ^ \mathrm{T}
$$

である。この$$\phi$$を**基底関数**（basis function）または**特徴量写像**（feature map）という。

$$\phi _ i$$の添字が 0 から始まったり 1 から始まったりしているが、切片の項にあたる定数写像$$\phi _ 0 (x) = 1$$はモデルに付け加えたり外したりがよくあるので、添字 0 を割り当てておくと楽である。つまり線形基底関数モデルでも

$$
\phi (x) = (\phi _ 0 (x), \phi _ 1 (x), \phi_2(x), \cdots , \phi _n(x)) ^ \mathrm{T}
$$

と書けば切片の項があるものとし、

$$
y = \sum _ {i = 0} ^ n w ^ \mathrm{T} \phi _ i (x) +\varepsilon
$$

と書けば切片ありのモデル、

$$
y = \sum _ {i = 1} ^ n w ^ \mathrm{T} \phi _ i (x) +\varepsilon
$$

と書けば切片なしのモデルであると約束しておくと数式を書き直す手間が省けてよい。

また特徴量写像$$\phi$$の定義域$$\mathcal{X}$$は任意の集合である。つまり必ずしも数値データである必要はなく、画像、文字列、グラフといったより一般の対象を入力に取ることができる（ここでは紹介しないがカーネル法の書籍などに具体例が載っているだろう）。

では具体的にいくつかの基底関数について見てみよう。

### 線形回帰モデル

線形回帰モデルは線形基底関数モデルの特殊ケースである。$$(2.2.2.4)$$式の

### 多項式回帰モデル

### ガウス基底関数モデル

### Bスプライン基底関数モデル

## 確率モデルと正則化

線形基底関数モデルの正則化は確率モデルからも導くことができ、重みベクトルに事前分布を設定してMAP推定を行うことと等価である。

$$
p(y, x, w) = p(y|x,w)p(x)p(w)
$$

$$
p(y, w|x)p(x) = p(y|x,w)p(x)p(w) \\
p(y,w|x)=p(y|x,w)p(w)
$$

$$
\underset{a \in \mathbb{R}}{\operatorname{arg} \operatorname{max}} \,\,p (\mathcal{D})
$$

