# ちょっとだけ振り返って

## 確率分布に対する操作

ベイズ推論では確率分布に対する様々な操作を駆使するので、確率分布を使うと何ができるのか、そして自分はいま何をしようとしているのかを整理しておかないと、いつの間にかまったく見当違いな計算をすることになる。ここでわかりやすくサイコロの例と理解を対応づけておこう。

### 0. 確率分布の表記のイメージ

あるサイコロの目の確率分布を$$p(x)$$とおくと、$$p(x)$$は「$$x$$の目がどれだけ出やすいか」を表している。「$$1$$の目がどれだけ出やすいか」は

$$
p(x=1)
$$

のように表記する。これは正規分布のような連続型確率分布でも同じことである。正規分布は

$$
p(x) = \frac{1}{\sqrt{2 \pi \sigma ^ 2}} \exp \left( - \frac{(x - \mu) ^ 2}{2 \sigma ^ 2} \right)
$$

と複雑な式になっているが、正規分布にしたがって$$2$$がどれだけ出やすいかは、単に$$x=2$$を代入して

$$
p(x=2) = \frac{1}{\sqrt{2 \pi \sigma ^ 2}} \exp \left( - \frac{(2 - \mu) ^ 2}{2 \sigma ^ 2} \right)
$$

として計算すればよい。ただし気を付けねばならないのは、ここで求めた$$p(x=2)$$は実数$$2$$が観測される確率ではない（確率分布$$p(x)$$は確率と違って$$p(x) > 1$$の値も取りうる）。**確率分布は**$$x$$**の観測されやすさの尺度ではあるが確率とはまた別の尺度である**。

確率と確率分布の関係を一応述べておくと、観測される値が集合$$X$$に属する確率$$P(X)$$は、確率分布$$p(x)$$を用いて

$$
P(X) = \int _ X p(x) d \mu(x)
$$

と書ける。右辺はルベーグ積分であるが、とりあえずここでは「確率と確率分布は別物である」「確率分布を積分すると確率が求まる」とだけ知っておけばよい。

$$p(x=2)$$はサイコロを投げる前であれば「$$2$$がどれだけ出やすそうか」を表す尺度と考えられるが、実際にサイコロを投げて$$2$$が出たあとで「この$$2$$の目がどれだけ出やすかったか」、つまり「$$2$$の目が出たのはどれくらいめずらしい現象であったか」を表す尺度とも考えられる。

### 1. サンプリングする

確率分布から値を取り出す（サイコロを振る）ことに相当。たとえばデータを観測することも、データを生成する確率分布からのサンプリングとみなせる。

$$
x \sim p(x)
$$

のように書く。左辺は実現値（サイコロを振って出た値）、右辺は確率分布（サイコロの目は何が出そうか）である。

**ただし機械学習においてはデータを生成する確率分布**$$p(x)$$**の素性は、通常はわからない**。つまりサイコロの目に何が書いてあるかもわからないし、どの目が出やすいかもわからない状況からのスタートになる。

### 2. サンプリングした結果からデータを生成する確率分布を予測する

サイコロの目に何が書いてあるか、どの目が出やすいかは、サイコロを振り続けてみれば（すなわちデータを観測していけば）次第にわかってくる。100回ほど振ってみて$$\{1,2,3,4\}$$ばかりが出るならば4面ダイスだろうし、$$\{1,2,3,4,5,6\}$$ばかりが出るならば6面ダイスだろう。さらに1000回ほど振ってみれば、重心が偏ったイカサマサイコロかどうかもわかってくる。

ここで重要になるのは「同じサイコロ」を100回振る（**実験環境を変えずにデータを観測する/予測したい対象に関するデータを集める**）ということだ。ダイスAを99回振っておいて「100回目には別のダイスBを振ります。さぁどんな目が出る？」と聞いても何が起こるかはまったくわからない。つまり、**予測したい対象に関係のないデータはいくら集めても意味がない**。

このことを数式で表してみよう。サイコロを100回振るということは確率変数が100個あることになる。つまりその同時確率分布は

$$
p(x _ 1, x _ 2, \ldots, x _ {100})
$$

である。同じサイコロを100回振る（1つのサイコロを100回振るか、それぞれが同じように振舞うよう目や材質などの条件を揃えた100個のサイコロを一斉に振る）ということは、それぞれの確率変数$$x _1, x _ 2, \ldots, x  _ {100}$$が同じ確率分布$$p(x)$$からサンプリングされることを意味する。つまり

$$
p(x) = p( x _ 1) = p (x _ 2) = \cdots = p (x _ {100})
$$

である。また、$$i$$番目のサイコロを振る試行は他のサイコロを振る試行には影響を与えないから、独立試行の確率で、

$$
p(x _ 1, x _ 2, \ldots, x _ {100}) = p(x _ 1) \times p(x _ 2) \times \cdots \times p(x _ {100}) = \prod _ {n = 1} ^ {100} p(x _ n)
$$

と書ける。もう1回ついでに同じサイコロを振るときの同時確率分布は

$$
p(x, x _ 1, x _ 2, \ldots, x _ {100}) = p(x)\prod _ {n = 1} ^ {100} p(x _ n)
$$

になる。

サイコロを100回振って確率変数$$x _ 1, x _ 2, \ldots, x _ {100}$$を観測したあとで101回目にどんな目が出そうかは、条件付き確率分布

$$
p(x|x _ 1, x _ 2, \ldots, x _ {100})
$$

で表せる。これは

**私たちはデータを生成する真の確率分布**$$p(x)$$**を知ることができない状況でも、その確率分布からなんらかの方法でサンプリングできるならば、観測されたデータ**$$\mathcal{D}$$**によって補強された条件付き確率**$$p(x|\mathcal{D})$$**は求めることができる**。

### 3. 未観測の値がどんな値を取りそうか予測する

「10回コインを投げ続けてすべて表が出た。さて11回目に表が出る確率は？」という問いはなかなか興味深い。誰かに聞かれたらどう答えるべきか、あなたも少し考えてみてほしい。

答えは出ただろうか？回答は以下の3パターンに分類される。

1. 確率は1/2である（コインを投げる試行は毎回独立だから）
2. 確率は1/2よりも大きい（コインがイカサマだろうから）
3. 確率は1/2よりも小さい（こんなに続けて表が出たのだから次は裏になりそうだ）

括弧の中の理由づけはともかく、論理的には、同じか、より大きいか、より小さいかの3択である。正解はどれだろうか。

**実は1,2の回答が正しい**。より正確には、どんなモデルを採用するか（何を信じて何を疑うか）で、どの回答が正しいかが変わる。コインの投げ方を変えれば3が正しい場合もある。順番に見ていこう。

#### 1. 確率は1/2である

「コインの表裏が出る確率はそれぞれ1/2である」という仮定を強く信じて疑わない場合、コインを投げる試行は毎回独立なので、次にコインを投げたときに表が出る確率は1/2のままでよい。

#### 2. 確率は1/2よりも大きい

「コインの表裏が出る確率はそれぞれ1/2である」という仮定を疑う場合、すなわち「コインの表裏が出る確率には偏りがあるかもしれない」と仮定した場合、次にコインを投げたときに表が出る確率は1/2よりもおそらく大きくなるだろう（これは実際にモデルを設計して計算してみないとわからない）。極端な話、映画『ダークナイト』のハービー・デントがやっていたように両面とも表のコインなのかもしれない。



#### 3. 確率は1/2よりも小さい

コインを順番に投げる場合この回答は誤りだが「11枚のコインを袋に入れてどのコインかわからない状態にして、1枚ずつ袋に戻さず10枚取り出したところすべて表だった。残りの1枚は？」という状況ではこの回答が正しい。

これは袋の中で混ぜたことでコインを取り出す試行が独立でなくなることによる影響である。

