# 2.2.1. 線形回帰

## 導入

まず簡単な例として線形回帰について解説する。

中学校でオームの法則を習ったことを覚えているだろうか。物体の電気抵抗$$R$$と物体にかける電圧$$V$$、流れる電流$$I$$の間には

$$
V=RI \tag{2.2.1.1}
$$

の関係が成り立つ。この法則を利用すれば物体の電気抵抗を測定でき、物体に所望の電圧をかけたり、電流を流したりできるようになる。

より一般化して$$(2.2.1.1)$$式を直線の方程式で書き直せば

$$
y=ax \tag{2.2.1.2}
$$

となる。電圧、電気抵抗、電流をそれぞれ$$y,a,x$$で置いただけである。機械学習の文化では$$x$$を入力\(input\)、$$y$$を出力\(output\)またはラベル\(label\)という。また$$a$$は重み\(weight\)と呼ばれ、本来なら$$w$$で表記することが多い。

$$(2.2.1.2)$$式のような関係性を持つことがわかっている、またはそのような関係性を持ちそうだと思われる現象は、重み$$a$$さえ求めれば入力から出力を予測できる。いまの状況ではどれくらいの電流を流したときにどれくらいの電圧がかかっているかが推測できる。

中学校や高校の範囲では、適当に1点$$(x,y)$$を測定してそれを代表とし、

$$
a = \frac{y}{x}
$$

とすれば求まるということになっていた。しかし電気抵抗を実際に何回か測定してみると、どの点を代表とするかによって求まる$$a$$の値は若干変わり、毎回同じ値になるとは限らない。これは物体の電気抵抗が温度に依存することや電気回路の接触などに起因するが、何が原因かはよくわからないことがほとんどだろう。

こういった「よくわからない観測値の揺らぎ」の扱いを得意とするのが**確率論**である。中学高校の範囲でならいくつかの観測点について求めた$$a$$を平均する、大学の学部教養レベルであれば最小二乗法を用いて$$a$$を求める、といった方法がある。

どちらも十分に実用的な方法であり、実際それで片付いてしまうことも多いが、いまは機械学習への導入に適した形で問題を解き直そう。

## モデリング

出力$$y$$と入力$$x$$の間には以下の関係があると仮定する。

$$
y = a x + \varepsilon \tag{2.2.1.3}
$$

この式は「$$y$$は基本的に$$ax$$で求まるが、よくわからない誤差$$\varepsilon$$を含む」という意味であり、理論値からのズレを$$\varepsilon$$という補正項に吸収させたことになる。ここで$$\varepsilon$$は平均$$0$$、分散$$\sigma ^ 2$$のガウス分布に従うものとする。

$$
\varepsilon \sim \mathcal{N} (\varepsilon | 0, \sigma ^ 2) \tag{2.2.1.4}
$$

この段階は**モデリング**\(modeling\)といって、**現象に対して人間が勝手に仮定を置くことであり、誤差が本当にガウス分布に従うかどうかはわからない**。しかし人間は現象に何かしら仮定を置かなければ対象を数学的に扱うことはできないし、また仮定の置き方に自由度があることで、人間やコンピュータが計算しやすい形の数式が導出されることもある。仮定がどれだけ正しそうかは、得られた結果を見てから考えればよく、もし満足いかない結果なら満足いくまで仮定を変更すればよい。

**ガウス分布**\(Gaussian distribution\)または**正規分布**\(normal distribution\)は以下の式で定義される連続確率分布である。

$$
\mathcal{N} (x | \mu, \sigma ^ 2) = \frac{1}{\sqrt{2 \pi \sigma ^ 2}} \exp \left(- \frac{(x - \mu) ^ 2}{2 \sigma ^ 2} \right) \tag{2.2.1.5}
$$

出力$$y$$は$$ax$$に$$\varepsilon$$を足したものなので、平均$$ax$$、分散$$\sigma ^ 2$$のガウス分布にしたがうものとみなせるから、その確率分布は

$$
p(y | x, a) = \mathcal{N} (y | ax, \sigma ^ 2) \tag{2.2.1.6}
$$

と表せる。最初なので丁寧に書いておくと、

$$
p(y | x , a) = \frac{1}{\sqrt{2 \pi \sigma ^ 2}} \exp \left(- \frac{(y - ax) ^ 2}{2 \sigma ^ 2} \right) \tag{2.2.1.7}
$$

ということだ。

観測した$$n$$個のデータ点の集合を

$$
\mathcal{D} = \left\{ (x ^ {(1)}, y ^ {(1)}), (x ^ {(2)}, y ^ {(2)}), \ldots, (x ^ {(n)}, y ^ {(n)}) \right \} \tag{2.2.1.8}
$$

とおく。$$\mathcal{D} ^ {(i)} = (x ^ {(i)}, y ^ {(i)})$$は$$i$$番目に観測されたデータ点を表し、入力の集合、出力の集合はそれぞれ

$$
\begin{aligned}
\mathcal{D} _ x = \left\{ x ^ {(1)}, x ^ {(2)},  \ldots, x ^ {(n)}\right \} \\
\mathcal{D} _ y = \left\{ y ^ {(1)}, y ^ {(2)},  \ldots, y ^ {(n)}\right \} \tag{2.2.1.9}
\end{aligned}
$$

で表す。

これらのデータ点が独立に観測されたと仮定すると、すべてのデータ点が観測される同時確率分布は

$$
\begin{aligned}
&p(\mathcal{D} _ y | \mathcal{D} _ x , a) \\
=\,&p(y ^ {(1)}, y ^ {(2)}, \ldots, y ^ {(n)} | x ^ {(1)}, x ^ {(2)}, \ldots, x ^ {(n)}, a) \\
=\,& p(y ^ {(1)} | x ^ {(1)}, a) \times p(y ^ {(2)} | x ^ {(2)}, a) \times  \cdots \times p(y ^ {(n)} | x ^ {(n)}, a) \\
=\,& \prod _ {i=1} ^ n p(y ^ {(i)} | x ^ {(i)}, a) 
\end{aligned} \tag{2.2.1.10}
$$

である。$$p(\mathcal{D} _y | \mathcal{D} _ x, a)$$はニュアンス的には「入力の集合$$\mathcal{D} _ x$$と重み$$a$$を与えたときの出力の集合$$\mathcal{D} _ y$$の観測されやすさ」を表す指標である。また入力$$\mathcal{D} _ x$$は実験後に動かすことはできないから、重み$$a$$を変数とみなして$$p(\mathcal{D} _y | \mathcal{D} _ x, a)$$を最大化すれば「いま手元にあるデータ$$\mathcal{D} _ y$$を生成しそうな、もっとも尤もらしい重み$$a$$」を求められることになる。





